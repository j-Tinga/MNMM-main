{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a689eb",
   "metadata": {},
   "source": [
    "Back Tracing is the processing of reverse engineering a function back to the original formula. Most researchers do this to understand which part of the mathematical foundations of the algorithmic function can be further improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea1a80",
   "metadata": {},
   "source": [
    "Example, you are given a code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b507ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1,x2,y1,y2):\n",
    "    z = ((x1-x2)**2+(y1-y2)**2)**(0.5)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa5f10",
   "metadata": {},
   "source": [
    "The formula above is a function for the Euclidean distance, back tracing (reverse engineering) it will yield:\n",
    "\n",
    "$$\n",
    "z = \\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f653f5",
   "metadata": {},
   "source": [
    "## Back tracing of the functions/methods from the Multionomial Expectation Maximizer \n",
    "Repository codebase by Adrien Biarnes: https://github.com/biarne-a/MNMM .\n",
    "\n",
    "Back tracing is mostly based on his medium article about the codebase Multinomial Mixture Model and its application in supermarket shoppers' segmentation: \n",
    "\n",
    "https://towardsdatascience.com/multinomial-mixture-model-for-supermarket-shoppers-segmentation-a-complete-tutorial-268974d905da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialExpectationMaximizer(k, restarts=10, rtol=1e-3)\n",
    "\n",
    "def __init__(self, K, rtol=1e-3, max_iter=100, restarts=10):\n",
    "        self._K = K\n",
    "        self._rtol = rtol\n",
    "        self._max_iter = max_iter\n",
    "        self._restarts = restarts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677eac28",
   "metadata": {},
   "source": [
    "The MultinomialExpectationMaximizer model object is intialized with the following **Parameters:**\n",
    "\n",
    "- **k:**\n",
    "    - Number of mixture components\n",
    "- **rtol:**\n",
    "    - Used for convergence checking.\n",
    "    - Stops the algorithm if the improvement in loss between iterations becomes too small.\n",
    "- **max_iter:**\n",
    "    - Sets a maximum number of iterations to prevent infinite running.\n",
    "- **restarts:**\n",
    "    - Specifies the number of times to restart the Expectation-Maximization (EM) procedure with different initializations.\n",
    "    - Helps find a better solution by exploring different starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _init_params(self, X):\n",
    "        C = X.shape[1]\n",
    "        weights = np.random.randint(1, 20, self._K)\n",
    "        alpha = weights / weights.sum()\n",
    "        beta = dirichlet.rvs([2 * C] * C, self._K)\n",
    "        return alpha, beta\n",
    "\n",
    "    def _train_once(self, X):\n",
    "        '''\n",
    "        Runs one full cycle of the EM algorithm\n",
    "\n",
    "        :param X: (N, C), matrix of counts\n",
    "        :return: The best parameters found along with the associated loss\n",
    "        '''\n",
    "        loss = float('inf')\n",
    "        alpha, beta = self._init_params(X)\n",
    "        gamma = None\n",
    "\n",
    "        for it in range(self._max_iter):\n",
    "            prev_loss = loss\n",
    "            gamma = self._e_step(X, alpha, beta)\n",
    "            alpha, beta = self._m_step(X, gamma)\n",
    "            loss = self._compute_vlb(X, alpha, beta, gamma)\n",
    "            print('Loss: %f' % loss)\n",
    "            if it > 0 and np.abs((prev_loss - loss) / prev_loss) < self._rtol:\n",
    "                    break\n",
    "        return alpha, beta, gamma, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9adcc6",
   "metadata": {},
   "source": [
    "## _train_once\n",
    "_train_once method will run one full cycle of iterations for the EM algorithm.\n",
    "It stars off initializing the parameters randomly (to avoid getting stuck in dead ends) with the beta parameters being initialized using the Dirichlet distribution which is conjugate to the multinomial distribution. This would produce a set of parameters that sum up to 1 (for every cluster) as required.\n",
    "\n",
    "To summarize each iteration:\n",
    "- It calculates probabilities of data points belonging to each cluster through **_e_step_**\n",
    "- Updates cluster weights and multinomial parameters based on responsibilities through **_m_step_**.\n",
    "- Tracks and outputs loss since the Expectation-Maximization (EM) algorithm is trying to maximize the lower bound through **_compute_vlb_**\n",
    "- Stops when either loss improvement is mininal (the _rtol_ parameter) or maximum iterations are reached (the _restarts_ parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bc0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _e_step(self, X, alpha, beta):\n",
    "    \"\"\"\n",
    "    Performs E-step on MNMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x C), matrix of counts\n",
    "    alpha: (K) or (NxK) in the case of individual weights, mixture component weights\n",
    "    beta: (K x C), multinomial categories weights\n",
    "\n",
    "    Returns:\n",
    "    gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "    \"\"\"\n",
    "    # Compute gamma\n",
    "    N = X.shape[0]\n",
    "    K = beta.shape[0]\n",
    "    weighted_multi_prob = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        weighted_multi_prob[:, k] = self._get_mixture_weight(alpha, k) * self._multinomial_prob(X, beta[k])\n",
    "\n",
    "    # To avoid division by 0\n",
    "    weighted_multi_prob[weighted_multi_prob == 0] = np.finfo(float).eps\n",
    "\n",
    "    denum = weighted_multi_prob.sum(axis=1)\n",
    "    gamma = weighted_multi_prob / denum.reshape(-1, 1)\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afe709",
   "metadata": {},
   "source": [
    "#### _e_step_\n",
    "The e_step method (Expectation step) computes a matrix of responsibilities, denoted as gamma (γ), where γ_ik represents the probability that data point i belongs to cluster k. This is calculated using the method's parameters: alpha (current mixture weights ) and beta (multinomial parameters ) for each cluster.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{weightedmultiprob}_{ik} &= \\text{getmixtureweight}(\\alpha, k) \\times \\text{multinomialprob}(X, \\beta_k) \\quad \\text{for } i = 1, 2, \\ldots, N \\text{ and } k = 1, 2, \\ldots, K \\\\\n",
    "weightedmultiprob[ \\text{weightedmultiprob} = 0] &= \\epsilon  \\\\\n",
    "\\text{denum}_i &= \\sum_{k=1}^{K} \\text{weightedmultiprob}_{ik} \\quad \\text{for } i = 1, 2, \\ldots, N \\\\\n",
    "\\gamma_{ik} &= \\frac{\\text{weightedmultiprob}_{ik}}{\\text{denum}_i} \\quad \\text{for } i = 1, 2, \\ldots, N \\text{ and } k = 1, 2, \\ldots, K\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66430ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _m_step(self, X, gamma):\n",
    "        \"\"\"\n",
    "        Performs M-step on MNMM model\n",
    "        Each input is numpy array:\n",
    "        X: (N x C), matrix of counts\n",
    "        gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "\n",
    "        Returns:\n",
    "        alpha: (K), mixture component weights\n",
    "        beta: (K x C), mixture categories weights\n",
    "        \"\"\"\n",
    "        # Compute alpha\n",
    "        alpha = self._m_step_alpha(gamma)\n",
    "\n",
    "        # Compute beta\n",
    "        beta = self._m_step_beta(X, gamma)\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "    def _m_step_alpha(self, gamma):\n",
    "        alpha = gamma.sum(axis=0) / gamma.sum()\n",
    "        return alpha\n",
    "\n",
    "    def _m_step_beta(self, X, gamma):\n",
    "        weighted_counts = gamma.T.dot(X)\n",
    "        beta = weighted_counts / weighted_counts.sum(axis=-1).reshape(-1, 1)\n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0e22c",
   "metadata": {},
   "source": [
    "#### _m_step_\n",
    "The m_step method (Maximation step) update the model parameters to maximize the expected complete-data log-likelihood, which was calculated in the E-step.\n",
    "\n",
    "For alpha, the new mixture weight for cluster k is calculated by averaging the responsibilities γ_ik across all data points i.\n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha_k &= \\frac{\\sum_{i=1}^{N} \\gamma_{ik}}{N} \\quad \\text{for } k = 1, 2, \\ldots, K\n",
    "\\end{align*}\n",
    "\n",
    "For beta, the new multinomial parameter for category j in cluster k is calculated by taking a weighted average of the corresponding counts X_ij across all data points i, weighted by their responsibilities γ_ik.\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta_{kj} &= \\frac{\\sum_{i=1}^{N} \\gamma_{ik} X_{ij}}{\\sum_{i=1}^{N} \\gamma_{ik}} \\quad \\text{for } k = 1, 2, \\ldots, K \\text{ and } j = 1, 2, \\ldots, C\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_vlb(self, X, alpha, beta, gamma):\n",
    "        \"\"\"\n",
    "        Computes the variational lower bound\n",
    "        X: (N x C), data points\n",
    "        alpha: (K) or (NxK) with individual weights, mixture component weights\n",
    "        beta: (K x C), multinomial categories weights\n",
    "        gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "\n",
    "        Returns value of variational lower bound\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for k in range(beta.shape[0]):\n",
    "            weights = gamma[:, k]\n",
    "            loss += np.sum(weights * (np.log(self._get_mixture_weight(alpha, k)) + self._multinomial_prob(X, beta[k], log=True)))\n",
    "            loss -= np.sum(weights * np.log(weights))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e872a",
   "metadata": {},
   "source": [
    "#### _compute_vlb_\n",
    "Calculates the variational lower bound (VLB), basically the loss, for the MMM.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{loss} = &\\sum_{k=1}^{K} \\left[ \\sum_{i=1}^{N} \\gamma_{ik} \\left( \\log(\\text{\\_get\\_mixture\\_weight}(\\alpha, k)) + \\text{\\_multinomial\\_prob}(X_i, \\beta_k, \\text{log=True}) \\right) \\right] \\\\\n",
    "&- \\sum_{i=1}^{N} \\gamma_{ik} \\log(\\gamma_{ik})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multinomial_prob(self, counts, beta, log=False):\n",
    "        \"\"\"\n",
    "        Evaluates the multinomial probability for a given vector of counts\n",
    "        counts: (N x C), matrix of counts\n",
    "        beta: (C), vector of multinomial parameters for a specific cluster k\n",
    "        Returns:\n",
    "        p: (N), scalar values for the probabilities of observing each count vector given the beta parameters\n",
    "        \"\"\"\n",
    "        n = counts.sum(axis=-1)\n",
    "        m = multinomial(n, beta)\n",
    "        if log:\n",
    "            return m.logpmf(counts)\n",
    "        return m.pmf(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35c606",
   "metadata": {},
   "source": [
    "The method multinomial_prob, as the comment say, evaluates the multinomial probability for a given vector of counts\n",
    "\n",
    "$$\n",
    "log(p_{i})= \\log(Multinomial(counts_{i}|n_{i},\\beta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9df6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(self, X_test, alpha, beta):\n",
    "        mn_probs = np.zeros(X_test.shape[0])\n",
    "        for k in range(beta.shape[0]):\n",
    "            mn_probs_k = self._get_mixture_weight(alpha, k) * self._multinomial_prob(X_test, beta[k])\n",
    "            mn_probs += mn_probs_k\n",
    "        mn_probs[mn_probs == 0] = np.finfo(float).eps\n",
    "        return np.log(mn_probs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959f1e0",
   "metadata": {},
   "source": [
    "#### _compute_log_likelihood_\n",
    "\n",
    "Calculates the log-likelihood of a given test dataset under a Multinomial Mixture Model using the mixture weights of the MMM (_alpha_) and the component probability distributions (_beta_).\n",
    "\n",
    "$$\n",
    "Computeloglikelihood(X_{test}, \\alpha,\\beta) = \\sum_{i=1}^{|X_{test}|} \\log (\\sum_{k=1}^{|\\beta|} GetMixtureWeight(\\alpha, k). MultinomialProb(X_{test[i]}, \\beta_{k}))\n",
    "$$\n",
    "    \n",
    "It returns the log-likelihood of the entire test data under the MMM, calculated as the sum of the log of the mixture probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f794a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bic(self, X_test, alpha, beta, log_likelihood=None):\n",
    "        if log_likelihood is None:\n",
    "            log_likelihood = self.compute_log_likelihood(X_test, alpha, beta)\n",
    "        N = X_test.shape[0]\n",
    "        return np.log(N) * (alpha.size + beta.size) - 2 * log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cd729",
   "metadata": {},
   "source": [
    "#### _compute_bic_\n",
    "Returns Bayesian Information Criterion (BIC) of the MMM using the mixture weights of the MMM (_alpha_) and the component probability distributions (_beta_) and pre-computed log-likelihood if available.\n",
    "$$\n",
    "BIC = -2.computeloglikelihood(x_{test},\\alpha, \\beta) + \\log(N) . (k_{\\alpha}+ k_{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e92c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_icl_bic(self, bic, gamma):\n",
    "        classification_entropy = -(np.log(gamma.max(axis=1))).sum()\n",
    "        return bic - classification_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45510bd0",
   "metadata": {},
   "source": [
    "#### _compute_icl_bic_\n",
    "Calculates the Integrated Complete-data Likelihood (ICL) BIC for the MMM using the BIC from _compute_bic_ the method and gamma\n",
    "$$\n",
    "ICL = BIC - \\sum_{i=1}^{N} \\log (max_{j})\\gamma_{ij}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
