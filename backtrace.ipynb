{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9df6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(self, X_test, alpha, beta):\n",
    "        mn_probs = np.zeros(X_test.shape[0])\n",
    "        for k in range(beta.shape[0]):\n",
    "            mn_probs_k = self._get_mixture_weight(alpha, k) * self._multinomial_prob(X_test, beta[k])\n",
    "            mn_probs += mn_probs_k\n",
    "        mn_probs[mn_probs == 0] = np.finfo(float).eps\n",
    "        return np.log(mn_probs).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959f1e0",
   "metadata": {},
   "source": [
    "$$\n",
    "Computeloglikelihood(X_{test}, \\alpha,\\beta) = \\sum_{i=1}^{|X_{test}|} \\log (\\sum_{k=1}^{|\\beta|} GetMixtureWeight(\\alpha, k). MultinomialProb(X_{test[i]}, \\beta_{k}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f794a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bic(self, X_test, alpha, beta, log_likelihood=None):\n",
    "        if log_likelihood is None:\n",
    "            log_likelihood = self.compute_log_likelihood(X_test, alpha, beta)\n",
    "        N = X_test.shape[0]\n",
    "        return np.log(N) * (alpha.size + beta.size) - 2 * log_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cd729",
   "metadata": {},
   "source": [
    "$$\n",
    "BIC = -2.computeloglikelihood(x_{test},\\alpha, \\beta) + \\log(N) . (k_{\\alpha}+ k_{\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e92c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_icl_bic(self, bic, gamma):\n",
    "        classification_entropy = -(np.log(gamma.max(axis=1))).sum()\n",
    "        return bic - classification_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45510bd0",
   "metadata": {},
   "source": [
    "$$\n",
    "ICL = BIC - \\sum_{i=1}^{N} \\log (max_{j})\\gamma_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50fac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    " def _multinomial_prob(self, counts, beta, log=False):\n",
    "        \"\"\"\n",
    "        Evaluates the multinomial probability for a given vector of counts\n",
    "        counts: (N x C), matrix of counts\n",
    "        beta: (C), vector of multinomial parameters for a specific cluster k\n",
    "        Returns:\n",
    "        p: (N), scalar values for the probabilities of observing each count vector given the beta parameters\n",
    "        \"\"\"\n",
    "        n = counts.sum(axis=-1)\n",
    "        m = multinomial(n, beta)\n",
    "        if log:\n",
    "            return m.logpmf(counts)\n",
    "        return m.pmf(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615731b",
   "metadata": {},
   "source": [
    "$$\n",
    "log(p_{i})= \\log(Multinomial(counts_{i}|n_{i},\\beta))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bc0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "{    def _e_step(self, X, alpha, beta):\n",
    "    \"\"\"\n",
    "    Performs E-step on MNMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x C), matrix of counts\n",
    "    alpha: (K) or (NxK) in the case of individual weights, mixture component weights\n",
    "    beta: (K x C), multinomial categories weights\n",
    "\n",
    "    Returns:\n",
    "    gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "    \"\"\"\n",
    "    # Compute gamma\n",
    "    N = X.shape[0]\n",
    "    K = beta.shape[0]\n",
    "    weighted_multi_prob = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        weighted_multi_prob[:, k] = self._get_mixture_weight(alpha, k) * self._multinomial_prob(X, beta[k])\n",
    "\n",
    "    # To avoid division by 0\n",
    "    weighted_multi_prob[weighted_multi_prob == 0] = np.finfo(float).eps\n",
    "\n",
    "    denum = weighted_multi_prob.sum(axis=1)\n",
    "    gamma = weighted_multi_prob / denum.reshape(-1, 1)\n",
    "\n",
    "    return gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7afe709",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{weightedmultiprob}_{ik} &= \\text{getmixtureweight}(\\alpha, k) \\times \\text{multinomialprob}(X, \\beta_k) \\quad \\text{for } i = 1, 2, \\ldots, N \\text{ and } k = 1, 2, \\ldots, K \\\\\n",
    "weightedmultiprob[ \\text{weightedmultiprob} = 0] &= \\epsilon  \\\\\n",
    "\\text{denum}_i &= \\sum_{k=1}^{K} \\text{weightedmultiprob}_{ik} \\quad \\text{for } i = 1, 2, \\ldots, N \\\\\n",
    "\\gamma_{ik} &= \\frac{\\text{weightedmultiprob}_{ik}}{\\text{denum}_i} \\quad \\text{for } i = 1, 2, \\ldots, N \\text{ and } k = 1, 2, \\ldots, K\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88792af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_vlb(self, X, alpha, beta, gamma):\n",
    "        \"\"\"\n",
    "        Computes the variational lower bound\n",
    "        X: (N x C), data points\n",
    "        alpha: (K) or (NxK) with individual weights, mixture component weights\n",
    "        beta: (K x C), multinomial categories weights\n",
    "        gamma: (N x K), posterior probabilities for objects clusters assignments\n",
    "\n",
    "        Returns value of variational lower bound\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for k in range(beta.shape[0]):\n",
    "            weights = gamma[:, k]\n",
    "            loss += np.sum(weights * (np.log(self._get_mixture_weight(alpha, k)) + self._multinomial_prob(X, beta[k], log=True)))\n",
    "            loss -= np.sum(weights * np.log(weights))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd7f81",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{loss} = &\\sum_{k=1}^{K} \\left[ \\sum_{i=1}^{N} \\gamma_{ik} \\left( \\log(\\text{{_get_mixture_weight}}(\\alpha, k)) + \\text{{_multinomial_prob}}(X_i, \\beta_k, \\text{{log=True}}) \\right) \\right. \\\n",
    "&\\left. - \\sum_{i=1}^{N} \\gamma_{ik} \\log(\\gamma_{ik}) \\right]\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
